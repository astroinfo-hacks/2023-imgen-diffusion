{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and data set creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 16:11:24.542206: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-05 16:11:25.456728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.visualization import astropy_mpl_style\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from astropy.io import fits\n",
    "from TNGDataSet import TNGDataSet\n",
    "plt.style.use(astropy_mpl_style)\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy scaling function\n",
    "def ScaleImage(img):\n",
    "    return img[0:500,0:500].astype('float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing function\n",
    "set to 0 for no traces up to 4 for alot of traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DIFF_TRACE\"]=\"0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset creation arguments and load the data set\n",
    "\n",
    "Dataset is only loaded the first time and will be stored locally in data_dir/tng_data_set folder\n",
    "If you wish to reload you need to delete this folder and relanch this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You use TensorFlow DType <dtype: 'float32'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to float32.\n",
      "WARNING:absl:You use TensorFlow DType <dtype: 'string'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to object.\n",
      "WARNING:absl:You use TensorFlow DType <dtype: 'int32'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /scratch/astroinfo2023/diffusion/tng_data_set/3.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 16:11:27.520672: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f56904051014e79a9ce58a27eae8247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dba005a385a42469e65f4bcd17ed993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /scratch/astroinfo2023/diffusion/tng_data_set/3.0.0.incomplete92QO08/tng_data_set-train.tfrecord*...â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "/scratch/astroinfo2023/diffusion/tng_data_set/3.0.0.incomplete92QO08/tng_data_set-train.tfrecord-00000-of-00001; Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#subsets = [tfds.Split.TRAIN,tfds.Split.VALIDATION, tfds.Split.TEST]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m subsets \u001b[38;5;241m=\u001b[39m [tfds\u001b[38;5;241m.\u001b[39mSplit\u001b[38;5;241m.\u001b[39mTRAIN,tfds\u001b[38;5;241m.\u001b[39mSplit\u001b[38;5;241m.\u001b[39mVALIDATION]\n\u001b[0;32m---> 11\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTNGDataSet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/scratch/astroinfo2023/diffusion/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/diffhack/lib/python3.11/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/micromamba/envs/diffhack/lib/python3.11/site-packages/tensorflow_datasets/core/load.py:640\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    634\u001b[0m dbuilder \u001b[38;5;241m=\u001b[39m _fetch_builder(\n\u001b[1;32m    635\u001b[0m     name,\n\u001b[1;32m    636\u001b[0m     data_dir,\n\u001b[1;32m    637\u001b[0m     builder_kwargs,\n\u001b[1;32m    638\u001b[0m     try_gcs,\n\u001b[1;32m    639\u001b[0m )\n\u001b[0;32m--> 640\u001b[0m \u001b[43m_download_and_prepare_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    643\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/micromamba/envs/diffhack/lib/python3.11/site-packages/tensorflow_datasets/core/load.py:499\u001b[0m, in \u001b[0;36m_download_and_prepare_builder\u001b[0;34m(dbuilder, download, download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m    498\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 499\u001b[0m   \u001b[43mdbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/diffhack/lib/python3.11/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/micromamba/envs/diffhack/lib/python3.11/site-packages/tensorflow_datasets/core/dataset_builder.py:646\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[1;32m    644\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir)\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 646\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[1;32m    652\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[1;32m    653\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[1;32m    654\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[1;32m    655\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[0;32m~/micromamba/envs/diffhack/lib/python3.11/site-packages/tensorflow_datasets/core/dataset_builder.py:1535\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1523\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m split_name, generator \u001b[38;5;129;01min\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   1524\u001b[0m       split_generators\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1525\u001b[0m       desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating splits...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1526\u001b[0m       unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m splits\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1527\u001b[0m       leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1528\u001b[0m   ):\n\u001b[1;32m   1529\u001b[0m     filename_template \u001b[38;5;241m=\u001b[39m naming\u001b[38;5;241m.\u001b[39mShardedFileTemplate(\n\u001b[1;32m   1530\u001b[0m         split\u001b[38;5;241m=\u001b[39msplit_name,\n\u001b[1;32m   1531\u001b[0m         dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1532\u001b[0m         data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[1;32m   1533\u001b[0m         filetype_suffix\u001b[38;5;241m=\u001b[39mpath_suffix,\n\u001b[1;32m   1534\u001b[0m     )\n\u001b[0;32m-> 1535\u001b[0m     future \u001b[38;5;241m=\u001b[39m \u001b[43msplit_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_split_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_shuffling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_shuffling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m     split_info_futures\u001b[38;5;241m.\u001b[39mappend(future)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;66;03m# Process the result of the beam pipeline.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/diffhack/lib/python3.11/site-packages/tensorflow_datasets/core/split_builder.py:341\u001b[0m, in \u001b[0;36mSplitBuilder.submit_split_generation\u001b[0;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Depending on the type of generator, we use the corresponding\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# `_build_from_xyz` method.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generator, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mIterable):\n\u001b[0;32m--> 341\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_from_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuild_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Otherwise, beam required\u001b[39;00m\n\u001b[1;32m    343\u001b[0m   unknown_generator_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    344\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid split generator value for split `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    345\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected generator or apache_beam object. Got: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    346\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(generator)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    347\u001b[0m   )\n",
      "File \u001b[0;32m~/micromamba/envs/diffhack/lib/python3.11/site-packages/tensorflow_datasets/core/split_builder.py:418\u001b[0m, in \u001b[0;36mSplitBuilder._build_from_generator\u001b[0;34m(self, split_name, generator, filename_template, disable_shuffling)\u001b[0m\n\u001b[1;32m    416\u001b[0m     utils\u001b[38;5;241m.\u001b[39mreraise(e, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to encode example:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    417\u001b[0m   writer\u001b[38;5;241m.\u001b[39mwrite(key, example)\n\u001b[0;32m--> 418\u001b[0m shard_lengths, total_size \u001b[38;5;241m=\u001b[39m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m split_info \u001b[38;5;241m=\u001b[39m splits_lib\u001b[38;5;241m.\u001b[39mSplitInfo(\n\u001b[1;32m    421\u001b[0m     name\u001b[38;5;241m=\u001b[39msplit_name,\n\u001b[1;32m    422\u001b[0m     shard_lengths\u001b[38;5;241m=\u001b[39mshard_lengths,\n\u001b[1;32m    423\u001b[0m     num_bytes\u001b[38;5;241m=\u001b[39mtotal_size,\n\u001b[1;32m    424\u001b[0m     filename_template\u001b[38;5;241m=\u001b[39mfilename_template,\n\u001b[1;32m    425\u001b[0m )\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _SplitInfoFuture(\u001b[38;5;28;01mlambda\u001b[39;00m: split_info)\n",
      "File \u001b[0;32m~/micromamba/envs/diffhack/lib/python3.11/site-packages/tensorflow_datasets/core/writer.py:267\u001b[0m, in \u001b[0;36mWriter.finalize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_spec \u001b[38;5;129;01min\u001b[39;00m shard_specs:\n\u001b[1;32m    264\u001b[0m   iterator \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mislice(\n\u001b[1;32m    265\u001b[0m       examples_generator, \u001b[38;5;241m0\u001b[39m, shard_spec\u001b[38;5;241m.\u001b[39mexamples_number\n\u001b[1;32m    266\u001b[0m   )\n\u001b[0;32m--> 267\u001b[0m   record_keys \u001b[38;5;241m=\u001b[39m \u001b[43m_write_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshard_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file_format\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m   \u001b[38;5;66;03m# No shard keys returned (e.g: TFRecord format), index cannot be\u001b[39;00m\n\u001b[1;32m    272\u001b[0m   \u001b[38;5;66;03m# created.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m record_keys:\n",
      "File \u001b[0;32m~/micromamba/envs/diffhack/lib/python3.11/site-packages/tensorflow_datasets/core/writer.py:161\u001b[0m, in \u001b[0;36m_write_examples\u001b[0;34m(path, iterator, file_format)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write_examples\u001b[39m(\n\u001b[1;32m    156\u001b[0m     path: epath\u001b[38;5;241m.\u001b[39mPathLike,\n\u001b[1;32m    157\u001b[0m     iterator: Iterable[type_utils\u001b[38;5;241m.\u001b[39mKeySerializedExample],\n\u001b[1;32m    158\u001b[0m     file_format: file_adapters\u001b[38;5;241m.\u001b[39mFileFormat \u001b[38;5;241m=\u001b[39m file_adapters\u001b[38;5;241m.\u001b[39mDEFAULT_FILE_FORMAT,\n\u001b[1;32m    159\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[file_adapters\u001b[38;5;241m.\u001b[39mExamplePositions]:\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Write examples from iterator in the given `file_format`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfile_adapters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mADAPTER_FOR_FORMAT\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfile_format\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/diffhack/lib/python3.11/site-packages/tensorflow_datasets/core/file_adapters.py:128\u001b[0m, in \u001b[0;36mTfRecordFileAdapter.write_examples\u001b[0;34m(cls, path, iterator)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_examples\u001b[39m(\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    116\u001b[0m     path: epath\u001b[38;5;241m.\u001b[39mPathLike,\n\u001b[1;32m    117\u001b[0m     iterator: Iterable[type_utils\u001b[38;5;241m.\u001b[39mKeySerializedExample],\n\u001b[1;32m    118\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[ExamplePositions]:\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Write examples from given iterator in given path.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFRecordWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, serialized_example \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m    130\u001b[0m       writer\u001b[38;5;241m.\u001b[39mwrite(serialized_example)\n",
      "File \u001b[0;32m~/micromamba/envs/diffhack/lib/python3.11/site-packages/tensorflow/python/lib/io/tf_record.py:294\u001b[0m, in \u001b[0;36mTFRecordWriter.__init__\u001b[0;34m(self, path, options)\u001b[0m\n\u001b[1;32m    291\u001b[0m   options \u001b[38;5;241m=\u001b[39m TFRecordOptions(compression_type\u001b[38;5;241m=\u001b[39moptions)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTFRecordWriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_record_writer_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: /scratch/astroinfo2023/diffusion/tng_data_set/3.0.0.incomplete92QO08/tng_data_set-train.tfrecord-00000-of-00001; Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "arg_dict = {\n",
    "    'train_percent':0.00001,\n",
    "    'val_percent':0.00001, \n",
    "    'test_percent':0.00001 , \n",
    "    'generation_verbosity' : 10 , \n",
    "    'Scaler_fcn':ScaleImage,\n",
    "    'band_filters' : ['CFHT_MEGACAM.U', 'SUBARU_HSC.G' ,'SUBARU_HSC.R' ,'CFHT_MEGACAM.R' ,'SUBARU_HSC.I', 'SUBARU_HSC.Z' ,'SUBARU_HSC.Y']\n",
    "    }\n",
    "#subsets = [tfds.Split.TRAIN,tfds.Split.VALIDATION, tfds.Split.TEST]\n",
    "subsets = [tfds.Split.TRAIN,tfds.Split.VALIDATION]\n",
    "ds = tfds.load('TNGDataSet', split=subsets, data_dir=\"/scratch/astroinfo2023/diffusion/\", builder_kwargs=arg_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recover the subsets you can recover them directly from the newly created dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set preparation\n",
    "def load_dataset(batch_size, noise_dist_std):\n",
    "\n",
    "  def pre_process(im):\n",
    "    \"\"\" Pre-processing function preparing data for denoising task\n",
    "    \"\"\"\n",
    "    # Cutout a portion of the map\n",
    "    x = tf.image.resize_with_crop_or_pad(im['img'][..., tf.newaxis], 128, 128)\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    x = tf.image.random_flip_up_down(x)\n",
    "    # Sample random Gaussian noise\n",
    "    u = tf.random.normal(tf.shape(x))\n",
    "    # Sample standard deviation of noise corruption\n",
    "    s = noise_dist_std * tf.random.normal((1, 1, 1))\n",
    "    # Create noisy image\n",
    "    y = x + s * u\n",
    "    return {'x':x, 'y':y, 'u':u,'s':s}\n",
    "\n",
    "  ds = train_dataset\n",
    "  ds = ds.shuffle(buffer_size=10*batch_size)\n",
    "  ds = ds.repeat()\n",
    "  ds = ds.map(pre_process)\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "  return ds.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_dset = load_dataset(1, 0.025)\n",
    "batch = next(cosmos_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "for i in range(4):\n",
    "    figure()\n",
    "    subplot(131)\n",
    "    imshow(batch['x'][i])\n",
    "    axis('off')\n",
    "    subplot(132)\n",
    "    imshow(batch['u'][i])\n",
    "    axis('off')\n",
    "    subplot(133)\n",
    "    imshow(batch['y'][i])\n",
    "    axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np.shape(batch['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(batch['y'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, so now we just need to train \n",
    "from models import SmallUResNet\n",
    "from normalization import SNParamsTree\n",
    "import haiku as hk\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hk.transform_with_state(lambda x, sigma, is_training=False: SmallUResNet()(x, sigma, is_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, state = model.init(jax.random.PRNGKey(0), jnp.zeros([1,128,128,1]), jnp.zeros([1,1,1,1]), is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(step):\n",
    "  \"\"\"Linear scaling rule optimized for 90 epochs.\"\"\"\n",
    "  steps_per_epoch = 40000 // 32\n",
    "\n",
    "  current_epoch = step / steps_per_epoch  # type: float\n",
    "  lr = (1.0 * 32) / 32\n",
    "  boundaries = jnp.array((20, 40, 60)) * steps_per_epoch\n",
    "  values = jnp.array([1., 0.1, 0.01, 0.001]) * lr\n",
    "\n",
    "  index = jnp.sum(boundaries < step)\n",
    "  return jnp.take(values, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.chain(\n",
    "  optax.adam(learning_rate=1e-3),\n",
    "  optax.scale_by_schedule(lr_schedule)\n",
    ")\n",
    "rng_seq = hk.PRNGSequence(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn_fn = hk.transform_with_state(lambda x: SNParamsTree(ignore_regex='[^?!.]*b', val=1.)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, sn_state = sn_fn.init(next(rng_seq), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, sn_state = sn_fn.apply(None, sn_state, None, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, state, rng_key, batch):\n",
    "    score, state = model.apply(params, state, rng_key, batch['y'], batch['s'], is_training=True)\n",
    "    loss = jnp.mean((batch['u'] + batch['s'] * (score))**2)\n",
    "    return loss, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jax.jit\n",
    "def update(params, state, sn_state, rng_key, opt_state, batch):\n",
    "    (loss, state), grads = jax.value_and_grad(loss_fn, has_aux=True)(params, state, rng_key, batch)\n",
    "    updates, new_opt_state = optimizer.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    new_params, new_sn_state = sn_fn.apply(None, sn_state, None, new_params)\n",
    "    return loss, new_params, state, new_sn_state, new_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.metrics import tensorboard\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tensorboard.SummaryWriter('models/score_model_0.025')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(2):\n",
    "    print(step)\n",
    "    loss, params, state, sn_state, opt_state = update(params, state, sn_state,\n",
    "                                                      next(rng_seq), opt_state,\n",
    "                                                      next(cosmos_dset))\n",
    "    summary_writer.scalar('train_loss', loss, step)\n",
    "    summary_writer.scalar('learning_rate', lr_schedule(step)*1e-3, step)\n",
    "    \n",
    "    if step%5000 ==0:\n",
    "        with open('models/score_model_0.025/model-%d.pckl'%step, 'wb') as file:\n",
    "            pickle.dump([params, state, sn_state], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_fn(y, s):\n",
    "    score, _ = model.apply(params, state, None, y, s.reshape((-1,1,1,1)), is_training=False)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = score_fn(batch['y'], batch['s'])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    figure(figsize=[20,5])\n",
    "    subplot(141)\n",
    "    imshow(batch['x'][i])\n",
    "    axis('off')\n",
    "    subplot(142)\n",
    "    imshow(batch['y'][i])\n",
    "    axis('off')\n",
    "    subplot(143)\n",
    "    imshow(score[i])\n",
    "    axis('off')\n",
    "    subplot(144)\n",
    "    imshow(batch['y'][i] + (batch['s']**2 * score)[i])\n",
    "    axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
